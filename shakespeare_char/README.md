Training on Shakespeare text using decoder only transformer with a simple Character only Tokenizer. Gets close to Karpathy's ~1.49 (we get to ~1.59) validation loss with room to get better. ##TODO: Since we are not using autotokenizer function the tokenizer is not saved in consistent manner as other datasets and infer does not work right. Mostly the experiment was to see if different tokenizers bottom out at different losses for different datasets. Usin GPT2 tokenizer seems to give model more room to learn since we are over training there which can probably be addressed by using more data.